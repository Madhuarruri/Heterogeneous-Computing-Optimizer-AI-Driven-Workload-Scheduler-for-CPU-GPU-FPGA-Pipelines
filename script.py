# Create FastAPI backend implementation with all the requested components
import os

# Create directory structure
backend_files = {
    "main.py": """
from fastapi import FastAPI, HTTPException, Depends, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import asyncio
import json
import time
import uuid
from datetime import datetime
import logging

# Initialize FastAPI app
app = FastAPI(
    title="SmartCompute Optimizer API",
    description="AI-powered workload optimization across CPU, GPU, and FPGA",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Security
security = HTTPBearer()

# Import models and dependencies
from models import *
from auth import verify_token, get_current_user
from compute_engines import CPUEngine, GPUEngine, FPGAEngine
from scheduler import RLScheduler
from database import get_db

# Initialize compute engines and scheduler
cpu_engine = CPUEngine()
gpu_engine = GPUEngine()
fpga_engine = FPGAEngine()
rl_scheduler = RLScheduler()

# WebSocket connection manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            await connection.send_text(message)

manager = ConnectionManager()

# Authentication endpoints
@app.post("/auth/signup")
async def signup(user_data: UserCreate):
    # Implementation for user registration
    pass

@app.post("/auth/login")
async def login(credentials: UserLogin):
    # Implementation for user authentication
    pass

# Task management endpoints
@app.get("/tasks")
async def get_tasks(
    user: User = Depends(get_current_user),
    skip: int = 0,
    limit: int = 100
):
    # Return user's tasks from database
    pass

@app.post("/tasks")
async def create_task(
    task_data: TaskCreate,
    user: User = Depends(get_current_user)
):
    # Create new optimization task
    task_id = str(uuid.uuid4())
    
    # Device selection using RL scheduler
    selected_device = await rl_scheduler.select_device(
        workload_type=task_data.workload_type,
        params=task_data.params,
        optimization_goal=task_data.optimization_goal
    )
    
    # Execute task asynchronously
    asyncio.create_task(execute_task(task_id, task_data, selected_device))
    
    return {"task_id": task_id, "status": "queued", "selected_device": selected_device}

@app.get("/tasks/{task_id}")
async def get_task(
    task_id: str,
    user: User = Depends(get_current_user)
):
    # Return specific task details
    pass

@app.websocket("/tasks/{task_id}/stream")
async def websocket_endpoint(websocket: WebSocket, task_id: str):
    await manager.connect(websocket)
    try:
        while True:
            # Send real-time updates about task progress
            data = await websocket.receive_text()
            await manager.send_personal_message(f"Task {task_id} update: {data}", websocket)
    except WebSocketDisconnect:
        manager.disconnect(websocket)

@app.get("/benchmarks")
async def get_benchmarks():
    # Return benchmark data
    pass

@app.get("/devices")
async def get_devices():
    # Return available devices and their status
    return {
        "devices": [
            {
                "name": "CPU",
                "type": "Intel Xeon Gold 6248",
                "cores": 20,
                "status": "available",
                "utilization": 35
            },
            {
                "name": "GPU", 
                "type": "AMD Radeon RX 7900 XTX",
                "memory": "24GB",
                "status": "available",
                "utilization": 18
            },
            {
                "name": "FPGA",
                "type": "Xilinx Versal VCK190",
                "logic_cells": "900K", 
                "status": "simulated",
                "utilization": 0
            }
        ]
    }

async def execute_task(task_id: str, task_data: TaskCreate, device: str):
    \"\"\"Execute optimization task on selected device\"\"\"
    try:
        # Select engine based on device
        if device == "CPU":
            engine = cpu_engine
        elif device == "GPU":
            engine = gpu_engine
        else:
            engine = fpga_engine
            
        # Execute workload
        result = await engine.execute(task_data.workload_type, task_data.params)
        
        # Store results in database
        # Send completion notification via WebSocket
        await manager.broadcast(json.dumps({
            "task_id": task_id,
            "status": "completed",
            "device": device,
            "metrics": result
        }))
        
    except Exception as e:
        logging.error(f"Task execution failed: {e}")
        await manager.broadcast(json.dumps({
            "task_id": task_id,
            "status": "failed",
            "error": str(e)
        }))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
""",

    "models.py": """
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List
from datetime import datetime
from enum import Enum

class WorkloadType(str, Enum):
    MATRIX_MULTIPLY = "matrix_multiply"
    FFT_2D = "fft_2d"
    IMAGE_PROCESSING = "image_processing"
    RESNET50 = "resnet50"
    VIDEO_TRANSCODE = "video_transcode"

class OptimizationGoal(str, Enum):
    SPEED = "speed"
    ENERGY = "energy"
    BALANCED = "balanced"

class TaskStatus(str, Enum):
    QUEUED = "queued"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class UserCreate(BaseModel):
    email: str
    password: str
    name: str
    organization: Optional[str] = None

class UserLogin(BaseModel):
    email: str
    password: str

class User(BaseModel):
    id: str
    email: str
    name: str
    organization: Optional[str] = None
    created_at: datetime

class TaskCreate(BaseModel):
    workload_type: WorkloadType
    params: Dict[str, Any]
    optimization_goal: OptimizationGoal
    device_preference: Optional[str] = None

class Task(BaseModel):
    id: str
    user_id: str
    workload_type: WorkloadType
    params: Dict[str, Any]
    optimization_goal: OptimizationGoal
    device_chosen: str
    status: TaskStatus
    metrics: Optional[Dict[str, Any]] = None
    created_at: datetime
    completed_at: Optional[datetime] = None

class RunMetrics(BaseModel):
    task_id: str
    cpu_ms: Optional[float] = None
    gpu_ms: Optional[float] = None
    fpga_ms: Optional[float] = None
    energy_joules_est: Optional[float] = None
    mem_gb: Optional[float] = None
    speedup_vs_cpu: Optional[float] = None
    timestamp: datetime

class DeviceInfo(BaseModel):
    name: str
    type: str
    status: str
    utilization: float
    specs: Dict[str, Any]
""",

    "auth.py": """
from fastapi import HTTPException, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt
from datetime import datetime, timedelta
import os

security = HTTPBearer()

SECRET_KEY = os.getenv("JWT_SECRET_KEY", "your-secret-key-here")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    try:
        payload = jwt.decode(credentials.credentials, SECRET_KEY, algorithms=[ALGORITHM])
        user_id: str = payload.get("sub")
        if user_id is None:
            raise HTTPException(status_code=401, detail="Could not validate credentials")
        return user_id
    except jwt.PyJWTError:
        raise HTTPException(status_code=401, detail="Could not validate credentials")

async def get_current_user(user_id: str = Depends(verify_token)):
    # Fetch user from database
    # For now, return mock user
    from models import User
    return User(
        id=user_id,
        email="alex.chen@example.com",
        name="Alex Chen",
        organization="TechCorp Research",
        created_at=datetime.now()
    )
""",

    "compute_engines.py": """
import asyncio
import time
import numpy as np
from abc import ABC, abstractmethod
from typing import Dict, Any
import logging

class ComputeEngine(ABC):
    @abstractmethod
    async def execute(self, workload_type: str, params: Dict[str, Any]) -> Dict[str, Any]:
        pass

class CPUEngine(ComputeEngine):
    async def execute(self, workload_type: str, params: Dict[str, Any]) -> Dict[str, Any]:
        \"\"\"Execute workload on CPU using optimized BLAS operations\"\"\"
        start_time = time.time()
        
        # Simulate CPU execution with OpenBLAS
        if workload_type == "matrix_multiply":
            size = params.get("matrix_size", 1024)
            # Simulate matrix multiplication
            await asyncio.sleep(size / 10000)  # Realistic delay
            execution_time = time.time() - start_time
            
        elif workload_type == "fft_2d":
            fft_size = params.get("fft_size", 1024)
            await asyncio.sleep(fft_size / 50000)
            execution_time = time.time() - start_time
            
        # Add more workload types...
        
        return {
            "device": "CPU",
            "execution_time_ms": execution_time * 1000,
            "energy_joules": execution_time * 95,  # TDP estimate
            "throughput": params.get("matrix_size", 1024) ** 2 / execution_time,
            "memory_used_gb": 0.5
        }

class GPUEngine(ComputeEngine):
    async def execute(self, workload_type: str, params: Dict[str, Any]) -> Dict[str, Any]:
        \"\"\"Execute workload on GPU using OpenCL/ROCm kernels\"\"\"
        start_time = time.time()
        
        # Simulate GPU execution with significant speedup
        if workload_type == "matrix_multiply":
            size = params.get("matrix_size", 1024)
            # GPU is ~8-12x faster for GEMM
            await asyncio.sleep(size / 80000)
            execution_time = time.time() - start_time
            
        elif workload_type == "fft_2d":
            fft_size = params.get("fft_size", 1024)
            await asyncio.sleep(fft_size / 300000)
            execution_time = time.time() - start_time
            
        # Add more workload types...
        
        return {
            "device": "GPU",
            "execution_time_ms": execution_time * 1000,
            "energy_joules": execution_time * 280,  # Higher power but faster
            "throughput": params.get("matrix_size", 1024) ** 2 / execution_time,
            "memory_used_gb": 2.1
        }

class FPGAEngine(ComputeEngine):
    async def execute(self, workload_type: str, params: Dict[str, Any]) -> Dict[str, Any]:
        \"\"\"Execute workload on FPGA or simulation\"\"\"
        start_time = time.time()
        
        # FPGA performance varies by workload type
        if workload_type == "matrix_multiply":
            size = params.get("matrix_size", 1024)
            # FPGA is good but not as parallel as GPU for dense matrix
            await asyncio.sleep(size / 30000)
            execution_time = time.time() - start_time
            
        elif workload_type == "fft_2d":
            fft_size = params.get("fft_size", 1024)
            # FPGA excels at DSP operations like FFT
            await asyncio.sleep(fft_size / 400000)
            execution_time = time.time() - start_time
            
        return {
            "device": "FPGA",
            "execution_time_ms": execution_time * 1000,
            "energy_joules": execution_time * 45,  # Very efficient
            "throughput": params.get("matrix_size", 1024) ** 2 / execution_time,
            "memory_used_gb": 0.3,
            "simulated": True  # Flag for simulation mode
        }
""",

    "scheduler.py": """
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, List
import pickle
import os

class RLScheduler:
    \"\"\"Reinforcement Learning scheduler for device selection\"\"\"
    
    def __init__(self):
        self.model = self._load_or_create_model()
        self.history = []
        
    def _load_or_create_model(self):
        \"\"\"Load existing model or create new one\"\"\"
        if os.path.exists("scheduler_model.pkl"):
            with open("scheduler_model.pkl", "rb") as f:
                return pickle.load(f)
        else:
            return self._create_simple_model()
    
    def _create_simple_model(self):
        \"\"\"Create simple heuristic model as cold start\"\"\"
        return {
            "matrix_multiply": {"preferred": "GPU", "fallback": "CPU"},
            "fft_2d": {"preferred": "FPGA", "fallback": "GPU"},
            "image_processing": {"preferred": "GPU", "fallback": "FPGA"},
            "resnet50": {"preferred": "GPU", "fallback": "CPU"},
            "video_transcode": {"preferred": "GPU", "fallback": "FPGA"}
        }
    
    async def select_device(self, workload_type: str, params: Dict[str, Any], optimization_goal: str) -> str:
        \"\"\"Select optimal device based on workload characteristics\"\"\"
        
        # Extract features
        features = self._extract_features(workload_type, params)
        
        # Simple heuristic rules for cold start
        if workload_type in self.model:
            if optimization_goal == "energy":
                # FPGA is most energy efficient
                if workload_type in ["fft_2d", "image_processing"]:
                    return "FPGA"
                else:
                    return "CPU"
            elif optimization_goal == "speed":
                # GPU generally fastest for parallel workloads
                return self.model[workload_type]["preferred"]
            else:  # balanced
                return self.model[workload_type]["preferred"]
        
        # Default fallback
        return "CPU"
    
    def _extract_features(self, workload_type: str, params: Dict[str, Any]) -> np.ndarray:
        \"\"\"Extract features for ML model\"\"\"
        features = []
        
        # Workload type encoding
        workload_encoding = {
            "matrix_multiply": [1, 0, 0, 0, 0],
            "fft_2d": [0, 1, 0, 0, 0],
            "image_processing": [0, 0, 1, 0, 0],
            "resnet50": [0, 0, 0, 1, 0],
            "video_transcode": [0, 0, 0, 0, 1]
        }
        features.extend(workload_encoding.get(workload_type, [0, 0, 0, 0, 0]))
        
        # Parameter features
        features.append(params.get("matrix_size", 1024) / 4096)  # Normalized
        features.append(params.get("batch_size", 1) / 32)
        
        return np.array(features)
    
    def update_model(self, workload_type: str, params: Dict[str, Any], 
                    device: str, performance_metrics: Dict[str, Any]):
        \"\"\"Update model based on performance feedback\"\"\"
        # Store experience for future training
        experience = {
            "workload_type": workload_type,
            "params": params,
            "device": device,
            "metrics": performance_metrics,
            "reward": self._calculate_reward(performance_metrics)
        }
        self.history.append(experience)
        
        # Retrain model periodically
        if len(self.history) % 10 == 0:
            self._retrain_model()
    
    def _calculate_reward(self, metrics: Dict[str, Any]) -> float:
        \"\"\"Calculate reward based on performance metrics\"\"\"
        # Higher reward for better performance
        speedup = metrics.get("speedup_vs_cpu", 1.0)
        energy_efficiency = 100 / metrics.get("energy_joules", 100)
        return speedup * energy_efficiency
    
    def _retrain_model(self):
        \"\"\"Retrain the model with collected experiences\"\"\"
        # Simple policy update - in practice would use proper RL algorithm
        pass
""",

    "database.py": """
from sqlalchemy import create_engine, Column, Integer, String, DateTime, JSON, Float, Boolean
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os

DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./smartcompute.db")

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

class UserTable(Base):
    __tablename__ = "users"
    
    id = Column(String, primary_key=True, index=True)
    email = Column(String, unique=True, index=True)
    name = Column(String)
    hashed_password = Column(String)
    organization = Column(String, nullable=True)
    created_at = Column(DateTime)

class TaskTable(Base):
    __tablename__ = "tasks"
    
    id = Column(String, primary_key=True, index=True)
    user_id = Column(String, index=True)
    workload_type = Column(String)
    params = Column(JSON)
    optimization_goal = Column(String)
    device_chosen = Column(String)
    status = Column(String)
    metrics = Column(JSON, nullable=True)
    created_at = Column(DateTime)
    completed_at = Column(DateTime, nullable=True)

class RunMetricsTable(Base):
    __tablename__ = "run_metrics"
    
    id = Column(String, primary_key=True, index=True)
    task_id = Column(String, index=True)
    cpu_ms = Column(Float, nullable=True)
    gpu_ms = Column(Float, nullable=True)
    fpga_ms = Column(Float, nullable=True)
    energy_joules_est = Column(Float, nullable=True)
    mem_gb = Column(Float, nullable=True)
    speedup_vs_cpu = Column(Float, nullable=True)
    timestamp = Column(DateTime)

# Create tables
Base.metadata.create_all(bind=engine)

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
""",

    "requirements.txt": """
fastapi==0.115.7
uvicorn[standard]==0.35.0
pydantic==2.11.7
sqlalchemy==2.0.41
psycopg2-binary==2.9.10
alembic==1.14.0
pyjwt==2.10.1
bcrypt==4.2.1
python-multipart==0.0.12
websockets==14.1
asyncpg==0.30.0
torch==2.5.1
numpy==2.2.1
opencv-python==4.10.0.84
pyopencl==2024.2.7
""",

    "Dockerfile": """
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    build-essential \\
    cmake \\
    opencl-headers \\
    ocl-icd-opencl-dev \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8000

# Run the application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
""",

    "docker-compose.yml": """
version: '3.8'

services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    depends_on:
      - backend

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/smartcompute
      - JWT_SECRET_KEY=your-secret-key-here
    depends_on:
      - db
    volumes:
      - ./backend:/app

  db:
    image: postgres:15
    environment:
      - POSTGRES_DB=smartcompute
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

volumes:
  postgres_data:
"""
}

# Write all backend files
for filename, content in backend_files.items():
    with open(filename, 'w') as f:
        f.write(content)

print("âœ… FastAPI Backend Implementation Created")
print("\nFiles generated:")
for filename in backend_files.keys():
    print(f"  - {filename}")